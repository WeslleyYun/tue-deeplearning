{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "workbook-task1.1.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1v_3Kdq4tqYC"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vlamen/tue-deeplearning/blob/main/assignments/assignment1/workbook-task1.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5YOR6q_Z2-b"
   },
   "source": [
    "TODO: Modify this cell to add your group name, group number and your names and student IDs\n",
    "\n",
    "Group: 51\n",
    "\n",
    "Authors:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuVJoCROtogK",
    "outputId": "f49fc160-4541-4c83-9737-f669f8e5af5a"
   },
   "source": [
    "import requests\n",
    "import io\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x0WIjQ7togW"
   },
   "source": [
    "### Training data set\n",
    "\n",
    "For task 1 of Assignment 1 you need to use a specific data set prepared using images from the [Omniglot dataset](https://github.com/brendenlake/omniglot). The provided training data set contains 18.800 binary images of handwritten characters of size (28,28). Each of these images depicts one of 893 different characters from 29 different alphabets. Each image is accompanied by a label that is encoded as an interger $y\\in\\{0, 1, ..., 892\\}$ that indicate the caracter depicted in the image. The following cell provides code that loads the data from hardcoded URLs.\n",
    "\n",
    "You can use the code in this cell to load the dataset or download the data set from the given URLs to your local drive (or your Google drive) and modify the code to load the data from another location. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLCixBNZtogX",
    "outputId": "33814293-f8d2-4cc8-f739-083b9e374b50"
   },
   "source": [
    "def load_numpy_arr_from_url(url):\n",
    "    \"\"\"\n",
    "    Loads a numpy array from surfdrive. \n",
    "    \n",
    "    Input:\n",
    "    url: Download link of dataset \n",
    "    \n",
    "    Outputs:\n",
    "    dataset: numpy array with input features or labels\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return np.load(io.BytesIO(response.content)) \n",
    "    \n",
    "    \n",
    "    \n",
    "# Downloading may take a while..\n",
    "train_x = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/tvQmLyY7MhVsADb/download')\n",
    "train_y = load_numpy_arr_from_url('https://surfdrive.surf.nl/files/index.php/s/z234AHrQqx9RVGH/download')\n",
    "\n",
    "print(f\"train_x shape: {train_x.shape}\")\n",
    "print(f\"train_y shape: {train_y.shape}\\n\")\n",
    "train_x_ge = (train_x.reshape((-1,784))/255)\n",
    "train_x_ge1 = torch.from_numpy(train_x_ge)\n",
    "train_x_re = train_x_ge1.type(torch.FloatTensor)\n",
    "train_y_re = torch.from_numpy(train_y)\n",
    "print(train_x_re)\n",
    "print(train_y_re)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape: (18800, 28, 28)\n",
      "train_y shape: (18800,)\n",
      "\n",
      "tensor([[0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        ...,\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039],\n",
      "        [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]])\n",
      "tensor([  1,   1,   1,  ..., 964, 964, 964])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": "940"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "len(np.unique(train_y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsyaadG4togZ"
   },
   "source": [
    "### Query data set\n",
    "\n",
    "For this task you need to use the following query data set. The dataset contains 100 sets of 6 images each. The images are also of hand written characters, however these characters are not present in the training data set. The characters in the query data set all come from the Greek alphabet that is not part of the set of alphabets in the training data. \n",
    "\n",
    "Each test set consists of 1 query image and 5 candidate images. All images are the same size (28x28). The test data is organized in two numpy arrays. One for the query images with shape (100, 1, 28, 28) and another for the candidate imagaes with shape (100, 5, 28, 28). \n",
    "\n",
    "The task is to develop a model that enables selecting the image which is depicting the same character as the anchor image out of 5 test images. These test images are declared in the `query_x` numpy array . \n",
    "\n",
    "Finally, we plot the first 5 cases in the query dataset. The first column corresponds with the anchor images of each of the 5 cases. All other images are test images from which the task is to recognize the anchor image. The image enclosed in a red box denotes the target image that your model should be able to recognize as the same class as the anchor image. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHZcmBr4togZ",
    "outputId": "e168005d-efc5-4919-9bed-ae1e6194bc05"
   },
   "source": [
    "query_dataset = load_numpy_arr_from_url(\"https://surfdrive.surf.nl/files/index.php/s/YGn5gb7unBEuCLB/download\")\n",
    "queries_true = load_numpy_arr_from_url(\"https://surfdrive.surf.nl/files/index.php/s/0sPeeIFB3W9RPZG/download\")\n",
    "\n",
    "queries, candidates_sets = np.split(query_dataset, [1], axis=1)\n",
    "\n",
    "print(f\"query images have shape: {queries.shape}\")\n",
    "print(f\"target sets have shape: {candidates_sets.shape}\")\n",
    "print(f\"ground truth: {queries_true.shape}\")"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query images have shape: (100, 1, 28, 28)\n",
      "target sets have shape: (100, 5, 28, 28)\n",
      "ground truth: (100,)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hDzMPaZMtoga",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "76282bfd-591f-4512-afb9-8328509e937e"
   },
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def plot_case(caseID):\n",
    "    \"\"\"\n",
    "    Plots a single sample of the query dataset\n",
    "    \n",
    "    Inputs\n",
    "    caseID: Integer between 0 and 99, each corresponding to a single sample in the query dataset \n",
    "    \"\"\"\n",
    "    \n",
    "    f, axes = plt.subplots(1, 6, figsize=(20,5))\n",
    "    \n",
    "    # plot anchor image\n",
    "    axes[0].imshow(queries[caseID, 0])\n",
    "    axes[0].set_title(f\"Anchor image case {caseID}\", fontsize=10)\n",
    "    \n",
    "    # show all test images images \n",
    "    [ax.imshow(candidates_sets[caseID, i]) for i, ax in enumerate(axes[1:])]\n",
    "    \n",
    "    \n",
    "    # Add the patch to the Axes\n",
    "    axes[queries_true[caseID]].add_patch(Rectangle((0,0),27,27,linewidth=2, edgecolor='r',facecolor='none'))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# plot the first five samples of the query datset\n",
    "[plot_case(caseID) for caseID in range(5)] ;"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ObMV6qoI0vc9"
   },
   "source": [
    "##\n",
    "# Setting up fit and loss functions as in tutorial 2.1\n",
    "# \n",
    "\n",
    "#Use a train test split on original train set to check model performance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(train_x_re, train_y_re)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    \n",
    "    output=model(xb)\n",
    "    loss = loss_func(output, yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    _, preds = torch.max(output, 1)\n",
    "    corrects = torch.sum(preds == yb.data)\n",
    "    \n",
    "    return loss.item(), corrects, len(xb)\n",
    "\n",
    "from tqdm import tqdm\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        \n",
    "        \n",
    "        # training process\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        sample_num=0\n",
    "        for xb, yb in train_dl:\n",
    "            \n",
    "            # forward\n",
    "            # backward and optimize only if in training phase\n",
    "            losses, corrects, nums = loss_batch(model, loss_func, xb, yb,opt)\n",
    "            \n",
    "            # statistics\n",
    "            running_loss += losses * xb.size(0)\n",
    "            running_corrects += corrects\n",
    "            sample_num+=nums\n",
    "            \n",
    "        train_loss = running_loss / sample_num\n",
    "        train_acc = running_corrects.double() / sample_num\n",
    "\n",
    "        \n",
    "        # validation process\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            sample_num=0\n",
    "            for xb, yb in valid_dl:\n",
    "                \n",
    "                # forward\n",
    "                losses, corrects, nums = loss_batch(model, loss_func, xb, yb)\n",
    "                \n",
    "                # statistics\n",
    "                running_loss += losses * xb.size(0)\n",
    "                running_corrects += corrects\n",
    "                sample_num+=nums\n",
    "\n",
    "            val_loss = running_loss / sample_num\n",
    "            val_acc = running_corrects.double()/ sample_num\n",
    "            \n",
    "            \n",
    "        # print the results\n",
    "        print(\n",
    "            f'EPOCH: {epoch+1:0>{len(str(epochs))}}/{epochs}',\n",
    "            end=' '\n",
    "        )\n",
    "        print(f'LOSS: {train_loss:.4f}',f'ACC: {train_acc:.4f} ',end=' ')\n",
    "        print(f'VAL-LOSS: {val_loss:.4f}',f'VAL-ACC: {val_acc:.4f} ',end='\\n')\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size=bs * 2),\n",
    "    )\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "valid_ds = TensorDataset(x_test, y_test)\n",
    "\n",
    "bs=64\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u-hPEtU74lDG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f07ec6f5-b5d3-435a-a68a-3fbe39f14d4c"
   },
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    return x.view(-1, 1, 28, 28)\n",
    "\n",
    "class model_mod(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(model_mod, self).__init__()\n",
    "\n",
    "        # Spatial transformer localization-network\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=7),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(8, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        # Regressor for the 3 * 2 affine matrix\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 3 * 2)\n",
    "        )\n",
    "\n",
    "        # Initialize the weights/bias with identity transformation\n",
    "        self.fc_loc[2].weight.data.fill_(0)\n",
    "        self.fc_loc[2].bias.data = torch.FloatTensor([1, 0, 0, 0, 1, 0])\n",
    "\n",
    "\n",
    "        self.front_layer = nn.Sequential(\n",
    "            Lambda(preprocess),\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 2\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "\n",
    "            nn.Linear(2304, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(p=0.5),\n",
    "            nn.Linear(1024, 965)\n",
    "        )\n",
    "\n",
    "\n",
    "    # Spatial transformer network forward function\n",
    "    def stn(self, x):\n",
    "        xs = self.front_layer(x)\n",
    "        xs = xs.view(-1, xs.size(0))\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "\n",
    "        grid = F.affine_grid(theta, x.size())\n",
    "        x = F.grid_sample(x, grid)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stn(x)\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "        # conv layers\n",
    "        x = self.front_layer(x)\n",
    "        return x\n",
    "\n",
    "    def get_embedding(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "model =  model_mod()\n",
    "lr = 0.1\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "epochs=50\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]c:\\users\\yunbo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py:3890: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "affine_grid only supports 4D and 5D sizes, for 2D and 3D affine transforms, respectively. Got size torch.Size([64, 784]).",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNotImplementedError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-29-f5dc1c630bb4>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    106\u001B[0m \u001B[0mloss_func\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcross_entropy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    107\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 108\u001B[1;33m \u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_func\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dl\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_dl\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    109\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-15-41b453118a7f>\u001B[0m in \u001B[0;36mfit\u001B[1;34m(epochs, model, loss_func, opt, train_dl, valid_dl)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[1;31m# forward\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[1;31m# backward and optimize only if in training phase\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mlosses\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrects\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnums\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_func\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0myb\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mopt\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m             \u001B[1;31m# statistics\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-15-41b453118a7f>\u001B[0m in \u001B[0;36mloss_batch\u001B[1;34m(model, loss_func, xb, yb, opt)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mloss_batch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mloss_func\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0myb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mopt\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m     \u001B[0moutput\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mloss_func\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0myb\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\yunbo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m    887\u001B[0m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    888\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 889\u001B[1;33m             \u001B[0mresult\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    890\u001B[0m         for hook in itertools.chain(\n\u001B[0;32m    891\u001B[0m                 \u001B[0m_global_forward_hooks\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-29-f5dc1c630bb4>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     91\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 93\u001B[1;33m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     94\u001B[0m         \u001B[1;34m\"\"\"Perform forward.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     95\u001B[0m         \u001B[1;31m# conv layers\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-29-f5dc1c630bb4>\u001B[0m in \u001B[0;36mstn\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     85\u001B[0m         \u001B[0mtheta\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtheta\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mview\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m3\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m         \u001B[0mgrid\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0maffine_grid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtheta\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m         \u001B[0mx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mF\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgrid_sample\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrid\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\yunbo\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36maffine_grid\u001B[1;34m(theta, size, align_corners)\u001B[0m\n\u001B[0;32m   3915\u001B[0m         \u001B[0mspatial_size\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0msize\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m3\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m  \u001B[1;31m# spatial dimension sizes\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3916\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3917\u001B[1;33m         raise NotImplementedError(\n\u001B[0m\u001B[0;32m   3918\u001B[0m             \u001B[1;34m\"affine_grid only supports 4D and 5D sizes, \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3919\u001B[0m             \u001B[1;34m\"for 2D and 3D affine transforms, respectively. \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNotImplementedError\u001B[0m: affine_grid only supports 4D and 5D sizes, for 2D and 3D affine transforms, respectively. Got size torch.Size([64, 784])."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szy27W6PI0sY",
    "outputId": "a72e5b9b-fc69-4782-d140-3b336298f3e3"
   },
   "source": [
    "#other implementation\n",
    "def preprocess(x, y):\n",
    "    return x.view(-1, 1, 28, 28), y\n",
    "\n",
    "\n",
    "class WrappedDataLoader:\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches:\n",
    "            yield (self.func(*b))\n",
    "\n",
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "    nn.AdaptiveAvgPool2d(1),\n",
    "    Lambda(lambda x: x.view(x.size(0), -1)),\n",
    "    nn.Linear(64, 128),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(128, 965),\n",
    "    \n",
    ")\n",
    "\n",
    "lr = 0.1\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "epochs=10\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RqyseRnwjt4R"
   },
   "source": [
    "## #TODO Modify this function such that you implement you character recognition algorithm here\n",
    "## The test code bellow will call this function with the following parameters \n",
    "## query - the query image (28, 28)\n",
    "## candidates - numpy array of candidate images, shape (5, 28, 28)\n",
    "## return - sorted array of the indexes of the images based on the similarty to the query image \n",
    "def test_model(query, candidates):\n",
    "  # TODO: dummy output that should be substituted by values produced by your solution\n",
    "  sorted_indexes = np.array([1, 2, 3, 4, 5]) \n",
    "\n",
    "  return sorted_indexes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMvs5VZuky4Q",
    "outputId": "c7a18bd0-450c-4934-c872-1ee9f5052343"
   },
   "source": [
    "## test top-1\n",
    "def test_top_1(query, candidates, query_true):\n",
    "  sorted_indexes = test_model(query, candidates)\n",
    "  return query_true == sorted_indexes[0]\n",
    "\n",
    "## test top-3\n",
    "def test_top_3(query, candidates, query_true):\n",
    "  sorted_indexes = test_model(query, candidates)\n",
    "  return np.isin(query_true, sorted_indexes[:3])\n",
    "\n",
    "top_1_res = np.array([test_top_1(a, b, c) for (a, b, c) in zip(queries, candidates_sets, queries_true)])\n",
    "top_3_res = np.array([test_top_3(a, b, c) for (a, b, c) in zip(queries, candidates_sets, queries_true)])\n",
    "\n",
    "top_1 = np.count_nonzero(top_1_res) / queries.shape[0]\n",
    "print(f\"top-1 accuracy: {top_1}\")\n",
    "\n",
    "top_3 = np.count_nonzero(top_3_res) / queries.shape[0]\n",
    "print(f\"top-3 accuracy: {top_3}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bHSVcJvDZ2-g"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}